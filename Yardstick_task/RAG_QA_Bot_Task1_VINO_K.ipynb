{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e35317a",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1 - RAG Model for QA Bot  \n",
    "**Intern Name:** VINO K\n",
    "\n",
    "This notebook demonstrates a Retrieval Augmented Generation (RAG) QA Bot for a business using:\n",
    "- OpenAI API for embeddings and generation\n",
    "- Pinecone DB for vector search\n",
    "- LangChain for orchestrating RAG\n",
    "\n",
    "Ensure your `.env` file contains the following keys:\n",
    "- `OPENAI_API_KEY`\n",
    "- `PINECONE_API_KEY`\n",
    "- `PINECONE_ENVIRONMENT`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67b59e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Index' from 'pinecone' (I:\\Anaconda\\Lib\\site-packages\\pinecone\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pinecone, Index\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings, ChatOpenAI\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Index' from 'pinecone' (I:\\Anaconda\\Lib\\site-packages\\pinecone\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Optimized RAG Model for QA Bot (Colab Notebook)\n",
    "\n",
    "This Colab notebook demonstrates an optimized Retrieval Augmented Generation (RAG) model\n",
    "for a Question-Answering (QA) bot, incorporating Contextual Compression using Relevance Filtering.\n",
    "It leverages:\n",
    "- OpenAI API for text embeddings and language model generation.\n",
    "- Pinecone as a vector database for efficient similarity search.\n",
    "- LangChain for orchestrating the RAG workflow (text splitting, document handling, summarization).\n",
    "\n",
    "Before running:\n",
    "1. Ensure you have an OpenAI API Key and a Pinecone API Key and Environment.\n",
    "2. Set these as environment variables (e.g., in a `.env` file or Colab secrets).\n",
    "\"\"\"\n",
    "\n",
    "# ✅ 1. Setup and Installation\n",
    "# Install necessary libraries. The -qU flags ensure quiet installation and upgrade.\n",
    "!pip install -qU openai pinecone-client langchain langchain-openai langchain-pinecone tiktoken python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, Index\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.summarize import load_summarize_chain # New import for summarization\n",
    "import time\n",
    "\n",
    "print(\"✅ Libraries installed and imported successfully by VINO K.\")\n",
    "\n",
    "# ✅ 2. Load Environment Variables\n",
    "# It's recommended to store API keys in a .env file for security.\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve API keys from environment variables\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_environment = os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "\n",
    "# Basic validation for API keys\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY missing.\")\n",
    "if not pinecone_api_key or not pinecone_environment:\n",
    "    raise ValueError(\"PINECONE_API_KEY or PINECONE_ENVIRONMENT missing.\")\n",
    "\n",
    "print(\"✅ Environment variables loaded.\")\n",
    "\n",
    "# ✅ 3. Initialize OpenAI and Pinecone Clients\n",
    "# Initialize Pinecone\n",
    "try:\n",
    "    pinecone = Pinecone(api_key=pinecone_api_key, environment=pinecone_environment)\n",
    "    print(\"✅ Pinecone initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Pinecone: {e}\")\n",
    "    raise\n",
    "\n",
    "# Initialize OpenAI Embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=openai_api_key, model=\"text-embedding-ada-002\")\n",
    "print(\"✅ OpenAI Embeddings model initialized.\")\n",
    "\n",
    "# Initialize OpenAI Chat model for generation (main LLM for answering)\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "print(\"✅ OpenAI Chat model initialized.\")\n",
    "\n",
    "# Initialize a lightweight LLM for summarization (for Contextual Compression)\n",
    "# Using gpt-3.5-turbo-instruct for summarization as suggested in the Canvas.\n",
    "llm_for_summarization = ChatOpenAI(openai_api_key=openai_api_key, model_name=\"gpt-3.5-turbo-instruct\", temperature=0.3)\n",
    "print(\"✅ Lightweight LLM for summarization initialized.\")\n",
    "\n",
    "# ✅ 4. Sample Business Data\n",
    "business_data_content = \"\"\"\n",
    "Prepared by VINO K\n",
    "\n",
    "Sample Product:\n",
    "- Name: AI Assistant\n",
    "- Price: $500\n",
    "- Features: Natural language understanding, Context awareness, Multi-turn dialogue\n",
    "\n",
    "Return Policy:\n",
    "- Returns accepted within 14 days if unopened.\n",
    "- For opened products, returns are not accepted due to software licensing.\n",
    "- Customers must provide original receipt for any return.\n",
    "- Refunds are processed within 5-7 business days.\n",
    "\n",
    "Customer Support:\n",
    "- Email: support@example.com\n",
    "- Phone: 1-800-AI-HELP\n",
    "- Hours: Monday - Friday, 9 AM - 5 PM EST\n",
    "\n",
    "Shipping Information:\n",
    "- Standard shipping: 3-5 business days.\n",
    "- Express shipping: 1-2 business days, additional cost.\n",
    "- Free standard shipping on orders over $1000.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"business_data.txt\", \"w\") as f:\n",
    "    f.write(business_data_content)\n",
    "\n",
    "print(\"✅ Sample business data loaded.\")\n",
    "\n",
    "# ✅ 5. Text Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, # Max characters per chunk\n",
    "    chunk_overlap=100, # Overlap between chunks to maintain context\n",
    "    length_function=len # Use character length\n",
    ")\n",
    "\n",
    "documents = [Document(page_content=business_data_content)]\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"✅ Document split into {len(chunks)} chunks.\")\n",
    "\n",
    "# ✅ 6. Pinecone Index Setup\n",
    "index_name = \"vino-task1-index\" # Using the same index name from your original code\n",
    "\n",
    "# Check if index already exists and delete if it does (for fresh start in Colab)\n",
    "if index_name in pinecone.list_indexes():\n",
    "    print(f\"Deleting existing index: {index_name}\")\n",
    "    pinecone.delete_index(index_name)\n",
    "    time.sleep(1) # Give a moment for index deletion\n",
    "\n",
    "# Create Pinecone index\n",
    "# The dimension must match the output dimension of your embedding model (1536 for text-embedding-ada-002)\n",
    "pinecone.create_index(index_name, dimension=1536, metric=\"cosine\")\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "print(f\"✅ Pinecone index '{index_name}' created and connected.\")\n",
    "\n",
    "# ✅ 7. Embedding and Upserting\n",
    "vectors_to_upsert = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_id = f\"id_{i}\"\n",
    "    # Embed the text content of the chunk\n",
    "    embedding = embeddings_model.embed_query(chunk.page_content)\n",
    "    # Store the original text content in metadata for retrieval\n",
    "    metadata = {\"text\": chunk.page_content} # Store original text in metadata\n",
    "    vectors_to_upsert.append((chunk_id, embedding, metadata))\n",
    "\n",
    "# Upsert embeddings to Pinecone in batches\n",
    "batch_size = 100 # Adjust batch size based on memory and network\n",
    "for i in range(0, len(vectors_to_upsert), batch_size):\n",
    "    batch = vectors_to_upsert[i:i + batch_size]\n",
    "    index.upsert(vectors=batch)\n",
    "    # print(f\"Upserted batch {i//batch_size + 1}/{(len(vectors_to_upsert) + batch_size - 1) // batch_size}\")\n",
    "\n",
    "print(\"✅ Embeddings upserted to Pinecone.\")\n",
    "print(f\"Pinecone index description: {index.describe_index_stats()}\")\n",
    "\n",
    "\n",
    "# ✅ 8. RAG QA Function with Contextual Compression\n",
    "def rag_qa_bot(query: str, top_k: int = 5, relevance_threshold: float = 0.7):\n",
    "    \"\"\"\n",
    "    Performs Retrieval Augmented Generation with Contextual Compression.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's question.\n",
    "        top_k (int): Number of top documents to retrieve initially from Pinecone.\n",
    "        relevance_threshold (float): Cosine similarity score threshold for filtering.\n",
    "\n",
    "    Returns:\n",
    "        str: The answer generated by the LLM.\n",
    "    \"\"\"\n",
    "    print(f\"🔎 Query: {query}\")\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "\n",
    "    # Retrieve documents from Pinecone\n",
    "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
    "\n",
    "    # 1. Relevance Filtering\n",
    "    # Filter documents based on a relevance score threshold\n",
    "    filtered_docs_for_summarization = []\n",
    "    print(f\"Filtering retrieved documents with relevance threshold > {relevance_threshold}...\")\n",
    "    for match in results.matches:\n",
    "        if match.score > relevance_threshold:\n",
    "            # Create a LangChain Document object for summarization chain\n",
    "            filtered_docs_for_summarization.append(Document(page_content=match.metadata['text'], metadata={\"score\": match.score}))\n",
    "            print(f\"  - Kept document (score: {match.score:.2f}): {match.metadata['text'][:70]}...\")\n",
    "        else:\n",
    "            print(f\"  - Discarded document (score: {match.score:.2f}): {match.metadata['text'][:70]}...\")\n",
    "\n",
    "    if not filtered_docs_for_summarization:\n",
    "        print(\"No relevant documents found after filtering.\")\n",
    "        return \"No relevant data found in my knowledge base to answer that question.\"\n",
    "\n",
    "    # 2. Contextual Compression using LangChain's summarize chain\n",
    "    # Using 'map_reduce' chain type for summarization over multiple documents\n",
    "    summary_chain = load_summarize_chain(llm_for_summarization, chain_type=\"map_reduce\")\n",
    "    print(f\"Compressing {len(filtered_docs_for_summarization)} relevant chunks...\")\n",
    "    try:\n",
    "        compressed_context = summary_chain.run(filtered_docs_for_summarization)\n",
    "        print(\"Context compressed successfully.\")\n",
    "        # print(f\"Compressed context:\\n{compressed_context[:200]}...\") # Uncomment to see compressed context\n",
    "    except Exception as e:\n",
    "        print(f\"Error during contextual compression: {e}\")\n",
    "        # Fallback to using uncompressed context if summarization fails\n",
    "        compressed_context = \"\\n\\n\".join([doc.page_content for doc in filtered_docs_for_summarization])\n",
    "        print(\"Falling back to uncompressed context.\")\n",
    "\n",
    "\n",
    "    # Augment prompt with the compressed context\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful QA assistant for Acme Innovations Inc.\n",
    "    Answer the following question based ONLY on the provided context.\n",
    "    If the answer is not found in the context, clearly state that you don't know or that the information is not available.\n",
    "\n",
    "    Context:\n",
    "    {compressed_context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    # print(f\"Full prompt sent to main LLM:\\n{prompt}\") # Uncomment to see the full prompt\n",
    "\n",
    "    # LLM Generation\n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "# ✅ 9. Test QA Bot\n",
    "queries = [\n",
    "    \"What is the price of AI Assistant?\",\n",
    "    \"What is the return policy for opened products?\",\n",
    "    \"How long does standard shipping take?\",\n",
    "    \"Who is VINO K?\", # This info is in the data (prepared by VINO K)\n",
    "    \"What is the capital of France?\" # This info is NOT in the data\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    answer = rag_qa_bot(q)\n",
    "    print(f\"🗨️ {q}\\n💡 {answer}\\n{'-'*30}\")\n",
    "    time.sleep(1) # Add a small delay for readability and to avoid hitting rate limits\n",
    "\n",
    "print(\"\\n✅ Task 1 completed professionally by VINO K with Contextual Compression!\")\n",
    "\n",
    "# Clean up: Delete the Pinecone index when done (optional, but good for managing resources)\n",
    "# print(f\"\\nDeleting Pinecone index '{index_name}'...\")\n",
    "# pinecone.delete_index(index_name)\n",
    "# print(\"Index deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e7b23-783e-4f4b-af1f-2c5e04871a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
