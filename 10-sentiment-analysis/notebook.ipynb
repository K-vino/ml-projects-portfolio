{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ­ Sentiment Analysis - IMDb Movie Reviews\n",
    "\n",
    "**Project**: Natural Language Processing - Text Classification  \n",
    "**Level**: Advanced  \n",
    "**Dataset**: IMDb Movie Reviews (Synthetic)  \n",
    "\n",
    "## ðŸ“‹ Project Overview\n",
    "\n",
    "This project performs sentiment analysis on movie reviews using NLP and deep learning techniques. We'll learn:\n",
    "\n",
    "- NLP fundamentals and text preprocessing\n",
    "- Tokenization, stemming, and lemmatization\n",
    "- Word embeddings (Word2Vec, GloVe)\n",
    "- LSTM/GRU models for text classification\n",
    "- Traditional ML vs deep learning for NLP\n",
    "\n",
    "Let's analyze movie sentiments! ðŸŽ¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning - Traditional\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"ðŸ§  TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ðŸŽ­ Ready for sentiment analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic movie reviews dataset\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"ðŸŽ¬ Generating synthetic movie reviews dataset...\")\n",
    "\n",
    "# Positive review templates and words\n",
    "positive_templates = [\n",
    "    \"This movie is absolutely {adj}! The {aspect} was {pos_word} and I {enjoyed} every minute.\",\n",
    "    \"I {loved} this film! {pos_word} {aspect} and {excellent} acting made it {amazing}.\",\n",
    "    \"What a {fantastic} movie! The {aspect} was {brilliant} and the story was {engaging}.\",\n",
    "    \"{excellent} film with {outstanding} {aspect}. I would {recommend} it to everyone!\",\n",
    "    \"This is one of the {best} movies I've seen. {pos_word} {aspect} and {great} direction.\"\n",
    "]\n",
    "\n",
    "negative_templates = [\n",
    "    \"This movie is {terrible}! The {aspect} was {bad} and I {hated} every minute.\",\n",
    "    \"I {disliked} this film. {bad} {aspect} and {poor} acting made it {awful}.\",\n",
    "    \"What a {disappointing} movie! The {aspect} was {boring} and the story was {confusing}.\",\n",
    "    \"{terrible} film with {poor} {aspect}. I would not {recommend} it to anyone.\",\n",
    "    \"This is one of the {worst} movies I've seen. {bad} {aspect} and {terrible} direction.\"\n",
    "]\n",
    "\n",
    "# Word lists\n",
    "positive_adjectives = ['amazing', 'fantastic', 'brilliant', 'excellent', 'outstanding', 'wonderful', 'superb', 'great']\n",
    "negative_adjectives = ['terrible', 'awful', 'horrible', 'disappointing', 'boring', 'confusing', 'poor', 'bad', 'worst']\n",
    "aspects = ['plot', 'acting', 'cinematography', 'soundtrack', 'direction', 'script', 'characters', 'dialogue']\n",
    "positive_verbs = ['loved', 'enjoyed', 'adored', 'appreciated']\n",
    "negative_verbs = ['hated', 'disliked', 'despised']\n",
    "\n",
    "# Generate reviews\n",
    "reviews = []\n",
    "labels = []\n",
    "n_reviews = 5000\n",
    "\n",
    "for i in range(n_reviews):\n",
    "    if i < n_reviews // 2:  # Positive reviews\n",
    "        template = random.choice(positive_templates)\n",
    "        review = template.format(\n",
    "            adj=random.choice(positive_adjectives),\n",
    "            aspect=random.choice(aspects),\n",
    "            pos_word=random.choice(positive_adjectives),\n",
    "            enjoyed=random.choice(positive_verbs),\n",
    "            loved=random.choice(positive_verbs),\n",
    "            excellent=random.choice(positive_adjectives),\n",
    "            amazing=random.choice(positive_adjectives),\n",
    "            fantastic=random.choice(positive_adjectives),\n",
    "            brilliant=random.choice(positive_adjectives),\n",
    "            engaging=random.choice(positive_adjectives),\n",
    "            outstanding=random.choice(positive_adjectives),\n",
    "            recommend='recommend',\n",
    "            best='best',\n",
    "            great=random.choice(positive_adjectives)\n",
    "        )\n",
    "        labels.append(1)  # Positive\n",
    "    else:  # Negative reviews\n",
    "        template = random.choice(negative_templates)\n",
    "        review = template.format(\n",
    "            terrible=random.choice(negative_adjectives),\n",
    "            aspect=random.choice(aspects),\n",
    "            bad=random.choice(negative_adjectives),\n",
    "            hated=random.choice(negative_verbs),\n",
    "            disliked=random.choice(negative_verbs),\n",
    "            poor=random.choice(negative_adjectives),\n",
    "            awful=random.choice(negative_adjectives),\n",
    "            disappointing=random.choice(negative_adjectives),\n",
    "            boring=random.choice(negative_adjectives),\n",
    "            confusing=random.choice(negative_adjectives),\n",
    "            recommend='recommend',\n",
    "            worst='worst'\n",
    "        )\n",
    "        labels.append(0)  # Negative\n",
    "    \n",
    "    reviews.append(review)\n",
    "\n",
    "# Add some noise and variation\n",
    "noise_words = ['really', 'very', 'quite', 'somewhat', 'definitely', 'absolutely', 'completely']\n",
    "for i in range(len(reviews)):\n",
    "    if random.random() < 0.3:  # 30% chance to add noise\n",
    "        noise = random.choice(noise_words)\n",
    "        reviews[i] = reviews[i].replace(' was ', f' was {noise} ')\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'review': reviews,\n",
    "    'sentiment': labels\n",
    "})\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nðŸŽ­ Movie reviews dataset created!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Total reviews: {len(df):,}\")\n",
    "print(f\"Positive reviews: {(df['sentiment'] == 1).sum():,}\")\n",
    "print(f\"Negative reviews: {(df['sentiment'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration\n",
    "print(\"ðŸ“Š Dataset Information:\")\n",
    "print(f\"Total reviews: {len(df):,}\")\n",
    "print(f\"Features: {list(df.columns)}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(f\"\\nðŸŽ­ Sentiment Distribution:\")\n",
    "print(f\"â€¢ Negative (0): {sentiment_counts[0]:,} ({sentiment_counts[0]/len(df):.1%})\")\n",
    "print(f\"â€¢ Positive (1): {sentiment_counts[1]:,} ({sentiment_counts[1]/len(df):.1%})\")\n",
    "\n",
    "# Review length analysis\n",
    "df['review_length'] = df['review'].str.len()\n",
    "df['word_count'] = df['review'].str.split().str.len()\n",
    "\n",
    "print(f\"\\nðŸ“ Review Statistics:\")\n",
    "print(f\"â€¢ Average review length: {df['review_length'].mean():.1f} characters\")\n",
    "print(f\"â€¢ Average word count: {df['word_count'].mean():.1f} words\")\n",
    "print(f\"â€¢ Shortest review: {df['review_length'].min()} characters\")\n",
    "print(f\"â€¢ Longest review: {df['review_length'].max()} characters\")\n",
    "\n",
    "# Sample reviews\n",
    "print(f\"\\nðŸ” Sample Reviews:\")\n",
    "print(\"\\nPositive Reviews:\")\n",
    "positive_samples = df[df['sentiment'] == 1]['review'].head(3)\n",
    "for i, review in enumerate(positive_samples, 1):\n",
    "    print(f\"{i}. {review}\")\n",
    "\n",
    "print(\"\\nNegative Reviews:\")\n",
    "negative_samples = df[df['sentiment'] == 0]['review'].head(3)\n",
    "for i, review in enumerate(negative_samples, 1):\n",
    "    print(f\"{i}. {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment and text analysis visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('ðŸŽ­ Sentiment Analysis - Text Exploration', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "bars1 = axes[0,0].bar(['Negative', 'Positive'], sentiment_counts.values, color=colors)\n",
    "axes[0,0].set_title('ðŸŽ­ Sentiment Distribution')\n",
    "axes[0,0].set_ylabel('Number of Reviews')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 50,\n",
    "                   f'{int(height):,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Review length distribution by sentiment\n",
    "positive_lengths = df[df['sentiment'] == 1]['review_length']\n",
    "negative_lengths = df[df['sentiment'] == 0]['review_length']\n",
    "\n",
    "axes[0,1].hist(positive_lengths, bins=30, alpha=0.7, label='Positive', color='#4ECDC4', density=True)\n",
    "axes[0,1].hist(negative_lengths, bins=30, alpha=0.7, label='Negative', color='#FF6B6B', density=True)\n",
    "axes[0,1].set_title('ðŸ“ Review Length Distribution')\n",
    "axes[0,1].set_xlabel('Review Length (characters)')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Word count distribution by sentiment\n",
    "positive_words = df[df['sentiment'] == 1]['word_count']\n",
    "negative_words = df[df['sentiment'] == 0]['word_count']\n",
    "\n",
    "axes[1,0].hist(positive_words, bins=20, alpha=0.7, label='Positive', color='#4ECDC4', density=True)\n",
    "axes[1,0].hist(negative_words, bins=20, alpha=0.7, label='Negative', color='#FF6B6B', density=True)\n",
    "axes[1,0].set_title('ðŸ“Š Word Count Distribution')\n",
    "axes[1,0].set_xlabel('Word Count')\n",
    "axes[1,0].set_ylabel('Density')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# Average review length by sentiment\n",
    "avg_lengths = df.groupby('sentiment')['review_length'].mean()\n",
    "bars2 = axes[1,1].bar(['Negative', 'Positive'], avg_lengths.values, color=colors)\n",
    "axes[1,1].set_title('ðŸ“ Average Review Length by Sentiment')\n",
    "axes[1,1].set_ylabel('Average Length (characters)')\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                   f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Key Observations:\")\n",
    "print(f\"â€¢ Dataset is balanced: {sentiment_counts[0]} negative, {sentiment_counts[1]} positive\")\n",
    "print(f\"â€¢ Average positive review length: {positive_lengths.mean():.1f} characters\")\n",
    "print(f\"â€¢ Average negative review length: {negative_lengths.mean():.1f} characters\")\n",
    "print(f\"â€¢ Average positive word count: {positive_words.mean():.1f} words\")\n",
    "print(f\"â€¢ Average negative word count: {negative_words.mean():.1f} words\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
