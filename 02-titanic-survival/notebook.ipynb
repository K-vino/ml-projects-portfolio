{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üö¢ Titanic Survival Predictor\n",
    "\n",
    "**Project**: Binary Classification - Predicting Titanic Passenger Survival  \n",
    "**Level**: Beginner  \n",
    "**Dataset**: Titanic Dataset (Seaborn built-in)  \n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "In this project, we'll predict whether passengers survived the Titanic disaster using machine learning. This classic dataset is perfect for learning:\n",
    "\n",
    "- Advanced data preprocessing and cleaning\n",
    "- Feature engineering and creation\n",
    "- Handling missing values\n",
    "- Working with categorical variables\n",
    "- Binary classification techniques\n",
    "\n",
    "The RMS Titanic sank on April 15, 1912, and this dataset contains information about passengers and their survival outcomes.\n",
    "\n",
    "Let's dive in! ‚öì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "Let's import all necessary libraries for our comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Advanced ML\n",
    "import xgboost as xgb\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"ü§ñ Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration\n",
    "\n",
    "Let's load the Titanic dataset and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset from seaborn\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "print(\"üö¢ Titanic dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"üîç First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"üìä Dataset Information:\")\n",
    "print(f\"Total passengers: {len(df)}\")\n",
    "print(f\"Features: {df.shape[1]}\")\n",
    "print(f\"Survival rate: {df['survived'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nüîç Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n‚ùå Missing values:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"üìà Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the relationships between different features and survival rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall survival distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Survival count\n",
    "survival_counts = df['survived'].value_counts()\n",
    "axes[0].pie(survival_counts.values, labels=['Did not survive', 'Survived'], \n",
    "           autopct='%1.1f%%', startangle=90, colors=['#FF6B6B', '#4ECDC4'])\n",
    "axes[0].set_title('üö¢ Overall Survival Distribution', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Survival by gender\n",
    "survival_by_sex = df.groupby('sex')['survived'].mean()\n",
    "bars = axes[1].bar(survival_by_sex.index, survival_by_sex.values, \n",
    "                   color=['#FF6B6B', '#4ECDC4'])\n",
    "axes[1].set_title('üë´ Survival Rate by Gender', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('Survival Rate')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Key Insight: Women had a {survival_by_sex['female']:.1%} survival rate vs {survival_by_sex['male']:.1%} for men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival analysis by passenger class and other factors\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('üîç Survival Analysis by Different Factors', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Survival by passenger class\n",
    "survival_by_class = df.groupby('pclass')['survived'].mean()\n",
    "bars1 = axes[0,0].bar(survival_by_class.index, survival_by_class.values, \n",
    "                      color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0,0].set_title('üé´ Survival Rate by Passenger Class')\n",
    "axes[0,0].set_xlabel('Passenger Class')\n",
    "axes[0,0].set_ylabel('Survival Rate')\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                   f'{height:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Age distribution by survival\n",
    "survived_ages = df[df['survived'] == 1]['age'].dropna()\n",
    "not_survived_ages = df[df['survived'] == 0]['age'].dropna()\n",
    "\n",
    "axes[0,1].hist(not_survived_ages, bins=30, alpha=0.7, label='Did not survive', color='#FF6B6B')\n",
    "axes[0,1].hist(survived_ages, bins=30, alpha=0.7, label='Survived', color='#4ECDC4')\n",
    "axes[0,1].set_title('üë∂ Age Distribution by Survival')\n",
    "axes[0,1].set_xlabel('Age')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Survival by embarkation port\n",
    "survival_by_embarked = df.groupby('embarked')['survived'].mean()\n",
    "bars2 = axes[1,0].bar(survival_by_embarked.index, survival_by_embarked.values, \n",
    "                      color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1,0].set_title('üö¢ Survival Rate by Embarkation Port')\n",
    "axes[1,0].set_xlabel('Embarkation Port')\n",
    "axes[1,0].set_ylabel('Survival Rate')\n",
    "axes[1,0].set_ylim(0, 1)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                   f'{height:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Fare distribution by survival\n",
    "survived_fare = df[df['survived'] == 1]['fare'].dropna()\n",
    "not_survived_fare = df[df['survived'] == 0]['fare'].dropna()\n",
    "\n",
    "axes[1,1].hist(not_survived_fare, bins=50, alpha=0.7, label='Did not survive', color='#FF6B6B')\n",
    "axes[1,1].hist(survived_fare, bins=50, alpha=0.7, label='Survived', color='#4ECDC4')\n",
    "axes[1,1].set_title('üí∞ Fare Distribution by Survival')\n",
    "axes[1,1].set_xlabel('Fare')\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_xlim(0, 200)  # Limit x-axis for better visualization\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "plt.figure(figsize=(10, 8))\n",
    "numerical_features = ['survived', 'pclass', 'age', 'sibsp', 'parch', 'fare']\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
    "            square=True, linewidths=0.5, fmt='.2f')\n",
    "plt.title('üî• Correlation Matrix - Numerical Features', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Key Correlations with Survival:\")\n",
    "survival_corr = correlation_matrix['survived'].sort_values(key=abs, ascending=False)[1:]\n",
    "for feature, corr in survival_corr.items():\n",
    "    print(f\"‚Ä¢ {feature}: {corr:.3f}\")\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Data Preprocessing and Feature Engineering\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now let's clean the data and create new features to improve our model's performance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create a copy of the dataset for preprocessing\\n\",\n",
    "    \"data = df.copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üîß Starting data preprocessing and feature engineering...\\\")\\n\",\n",
    "    \"print(f\\\"Original dataset shape: {data.shape}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 1. Handle missing values in 'age'\\n\",\n",
    "    \"# Fill missing ages with median age by passenger class and gender\\n\",\n",
    "    \"age_median = data.groupby(['pclass', 'sex'])['age'].median()\\n\",\n",
    "    \"for pclass in data['pclass'].unique():\\n\",\n",
    "    \"    for sex in data['sex'].unique():\\n\",\n",
    "    \"        mask = (data['pclass'] == pclass) & (data['sex'] == sex) & (data['age'].isnull())\\n\",\n",
    "    \"        data.loc[mask, 'age'] = age_median[pclass, sex]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"‚úÖ Age missing values filled: {data['age'].isnull().sum()} remaining\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 2. Handle missing values in 'embarked'\\n\",\n",
    "    \"# Fill with the most common port\\n\",\n",
    "    \"most_common_port = data['embarked'].mode()[0]\\n\",\n",
    "    \"data['embarked'].fillna(most_common_port, inplace=True)\\n\",\n",
    "    \"print(f\\\"‚úÖ Embarked missing values filled with '{most_common_port}': {data['embarked'].isnull().sum()} remaining\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 3. Create new features\\n\",\n",
    "    \"print(\\\"\\\\nüõ†Ô∏è Creating new features...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Family size\\n\",\n",
    "    \"data['family_size'] = data['sibsp'] + data['parch'] + 1\\n\",\n",
    "    \"print(f\\\"‚úÖ Created 'family_size' feature\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Is alone\\n\",\n",
    "    \"data['is_alone'] = (data['family_size'] == 1).astype(int)\\n\",\n",
    "    \"print(f\\\"‚úÖ Created 'is_alone' feature\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Age groups\\n\",\n",
    "    \"data['age_group'] = pd.cut(data['age'], bins=[0, 12, 18, 35, 60, 100], \\n\",\n",
    "    \"                          labels=['Child', 'Teen', 'Adult', 'Middle_age', 'Senior'])\\n\",\n",
    "    \"print(f\\\"‚úÖ Created 'age_group' feature\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Fare bins\\n\",\n",
    "    \"data['fare_bin'] = pd.qcut(data['fare'], q=4, labels=['Low', 'Medium', 'High', 'Very_High'])\\n\",\n",
    "    \"print(f\\\"‚úÖ Created 'fare_bin' feature\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Final dataset shape: {data.shape}\\\")\\n\",\n",
    "    \"print(f\\\"üìä New features added: {data.shape[1] - df.shape[1]}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Prepare features for machine learning\\n\",\n",
    "    \"print(\\\"üîß Preparing features for machine learning...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Select features for modeling\\n\",\n",
    "    \"feature_columns = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', \\n\",\n",
    "    \"                  'family_size', 'is_alone']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create feature matrix\\n\",\n",
    "    \"X = data[feature_columns].copy()\\n\",\n",
    "    \"y = data['survived']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Encode categorical variables\\n\",\n",
    "    \"label_encoders = {}\\n\",\n",
    "    \"categorical_features = ['sex', 'embarked']\\n\",\n",
    "    \"\\n\",\n",
    "    \"for feature in categorical_features:\\n\",\n",
    "    \"    le = LabelEncoder()\\n\",\n",
    "    \"    X[feature] = le.fit_transform(X[feature])\\n\",\n",
    "    \"    label_encoders[feature] = le\\n\",\n",
    "    \"    print(f\\\"‚úÖ Encoded '{feature}': {dict(zip(le.classes_, le.transform(le.classes_)))}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Final feature matrix shape: {X.shape}\\\")\\n\",\n",
    "    \"print(f\\\"üìä Target variable shape: {y.shape}\\\")\\n\",\n",
    "    \"print(f\\\"üìä Features used: {list(X.columns)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Model Building and Training\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's train multiple classification algorithms and compare their performance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Split the data\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    X, y, test_size=0.2, random_state=42, stratify=y\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÇÔ∏è Data split completed:\\\")\\n\",\n",
    "    \"print(f\\\"Training set: {X_train.shape[0]} samples\\\")\\n\",\n",
    "    \"print(f\\\"Testing set: {X_test.shape[0]} samples\\\")\\n\",\n",
    "    \"print(f\\\"Training survival rate: {y_train.mean():.1%}\\\")\\n\",\n",
    "    \"print(f\\\"Testing survival rate: {y_test.mean():.1%}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Scale the features\\n\",\n",
    "    \"scaler = StandardScaler()\\n\",\n",
    "    \"X_train_scaled = scaler.fit_transform(X_train)\\n\",\n",
    "    \"X_test_scaled = scaler.transform(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n‚öñÔ∏è Feature scaling completed!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = {\\n\",\n",
    "    \"    'Logistic Regression': LogisticRegression(random_state=42),\\n\",\n",
    "    \"    'Decision Tree': DecisionTreeClassifier(random_state=42),\\n\",\n",
    "    \"    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\\n\",\n",
    "    \"    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\\n\",\n",
    "    \"    'SVM': SVC(probability=True, random_state=42),\\n\",\n",
    "    \"    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"ü§ñ Models initialized:\\\")\\n\",\n",
    "    \"for name in models.keys():\\n\",\n",
    "    \"    print(f\\\"‚Ä¢ {name}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
