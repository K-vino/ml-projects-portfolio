{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ AutoML + SHAP Explainer\n",
    "\n",
    "**Project**: Automated Machine Learning + Explainable AI  \n",
    "**Level**: Expert  \n",
    "**Dataset**: Multiple datasets for comparison  \n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "This project implements automated machine learning (AutoML) with model explainability using SHAP. We'll learn:\n",
    "\n",
    "- AutoML frameworks and techniques\n",
    "- Automated feature engineering\n",
    "- Hyperparameter optimization\n",
    "- SHAP for model explainability\n",
    "- Production-ready ML pipelines\n",
    "\n",
    "Let's automate machine learning with explainable AI! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# AutoML Libraries\n",
    "try:\n",
    "    import autosklearn.classification\n",
    "    import autosklearn.regression\n",
    "    AUTOSKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    AUTOSKLEARN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Auto-sklearn not available. Using alternative AutoML approach.\")\n",
    "\n",
    "try:\n",
    "    from tpot import TPOTClassifier, TPOTRegressor\n",
    "    TPOT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TPOT_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è TPOT not available.\")\n",
    "\n",
    "try:\n",
    "    import pycaret\n",
    "    from pycaret.classification import *\n",
    "    from pycaret.regression import *\n",
    "    PYCARET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYCARET_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è PyCaret not available.\")\n",
    "\n",
    "# Explainable AI\n",
    "import shap\n",
    "try:\n",
    "    import lime\n",
    "    from lime.lime_tabular import LimeTabularExplainer\n",
    "    LIME_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIME_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è LIME not available.\")\n",
    "\n",
    "# Traditional ML (fallback)\n",
    "from sklearn.datasets import load_breast_cancer, load_wine, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Hyperparameter optimization\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Core libraries imported successfully!\")\n",
    "print(f\"ü§ñ AutoML Libraries Available:\")\n",
    "print(f\"   ‚Ä¢ Auto-sklearn: {AUTOSKLEARN_AVAILABLE}\")\n",
    "print(f\"   ‚Ä¢ TPOT: {TPOT_AVAILABLE}\")\n",
    "print(f\"   ‚Ä¢ PyCaret: {PYCARET_AVAILABLE}\")\n",
    "print(f\"   ‚Ä¢ SHAP: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ LIME: {LIME_AVAILABLE}\")\n",
    "print(f\"üöÄ Ready for AutoML + Explainable AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple datasets for comprehensive AutoML testing\n",
    "print(\"üìä Loading multiple datasets for AutoML comparison...\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "# 1. Breast Cancer Dataset (Classification)\n",
    "cancer_data = load_breast_cancer()\n",
    "datasets['breast_cancer'] = {\n",
    "    'X': pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names),\n",
    "    'y': cancer_data.target,\n",
    "    'target_names': cancer_data.target_names,\n",
    "    'type': 'classification',\n",
    "    'description': 'Breast Cancer Wisconsin (Diagnostic)'\n",
    "}\n",
    "\n",
    "# 2. Wine Dataset (Classification)\n",
    "wine_data = load_wine()\n",
    "datasets['wine'] = {\n",
    "    'X': pd.DataFrame(wine_data.data, columns=wine_data.feature_names),\n",
    "    'y': wine_data.target,\n",
    "    'target_names': wine_data.target_names,\n",
    "    'type': 'classification',\n",
    "    'description': 'Wine Recognition Dataset'\n",
    "}\n",
    "\n",
    "# 3. Diabetes Dataset (Regression)\n",
    "diabetes_data = load_diabetes()\n",
    "datasets['diabetes'] = {\n",
    "    'X': pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names),\n",
    "    'y': diabetes_data.target,\n",
    "    'target_names': ['diabetes_progression'],\n",
    "    'type': 'regression',\n",
    "    'description': 'Diabetes Dataset'\n",
    "}\n",
    "\n",
    "# 4. Synthetic Dataset (Classification)\n",
    "from sklearn.datasets import make_classification\n",
    "X_synthetic, y_synthetic = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "    n_classes=2, random_state=42\n",
    ")\n",
    "feature_names_synthetic = [f'feature_{i}' for i in range(20)]\n",
    "datasets['synthetic'] = {\n",
    "    'X': pd.DataFrame(X_synthetic, columns=feature_names_synthetic),\n",
    "    'y': y_synthetic,\n",
    "    'target_names': ['class_0', 'class_1'],\n",
    "    'type': 'classification',\n",
    "    'description': 'Synthetic Classification Dataset'\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Datasets loaded successfully:\")\n",
    "for name, data in datasets.items():\n",
    "    print(f\"‚Ä¢ {name}: {data['description']}\")\n",
    "    print(f\"  - Shape: {data['X'].shape}\")\n",
    "    print(f\"  - Type: {data['type']}\")\n",
    "    print(f\"  - Classes: {len(data['target_names'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset exploration and visualization\n",
    "def explore_dataset(name, dataset_info):\n",
    "    \"\"\"Explore a single dataset\"\"\"\n",
    "    print(f\"üîç Exploring {name} dataset...\")\n",
    "    \n",
    "    X, y = dataset_info['X'], dataset_info['y']\n",
    "    \n",
    "    print(f\"Dataset: {dataset_info['description']}\")\n",
    "    print(f\"Samples: {X.shape[0]:,}\")\n",
    "    print(f\"Features: {X.shape[1]}\")\n",
    "    print(f\"Task: {dataset_info['type']}\")\n",
    "    \n",
    "    if dataset_info['type'] == 'classification':\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"Classes: {len(unique_classes)}\")\n",
    "        for cls, count in zip(unique_classes, counts):\n",
    "            class_name = dataset_info['target_names'][cls] if cls < len(dataset_info['target_names']) else f'class_{cls}'\n",
    "            print(f\"  ‚Ä¢ {class_name}: {count} ({count/len(y):.1%})\")\n",
    "    else:\n",
    "        print(f\"Target range: {y.min():.2f} - {y.max():.2f}\")\n",
    "        print(f\"Target mean: {y.mean():.2f} ¬± {y.std():.2f}\")\n",
    "    \n",
    "    print(f\"Missing values: {X.isnull().sum().sum()}\")\n",
    "    print(f\"Duplicate rows: {X.duplicated().sum()}\")\n",
    "    print()\n",
    "\n",
    "# Explore all datasets\n",
    "for name, dataset_info in datasets.items():\n",
    "    explore_dataset(name, dataset_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom AutoML Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoML:\n",
    "    \"\"\"\n",
    "    A simple AutoML implementation that:\n",
    "    1. Tries multiple algorithms\n",
    "    2. Performs hyperparameter tuning\n",
    "    3. Selects the best model\n",
    "    4. Provides model explanations with SHAP\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task_type='classification', time_limit=300, cv_folds=5):\n",
    "        self.task_type = task_type\n",
    "        self.time_limit = time_limit\n",
    "        self.cv_folds = cv_folds\n",
    "        self.best_model = None\n",
    "        self.best_score = None\n",
    "        self.results = []\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Define model candidates\n",
    "        if task_type == 'classification':\n",
    "            self.models = {\n",
    "                'RandomForest': {\n",
    "                    'model': RandomForestClassifier(random_state=42),\n",
    "                    'params': {\n",
    "                        'n_estimators': randint(50, 200),\n",
    "                        'max_depth': randint(3, 20),\n",
    "                        'min_samples_split': randint(2, 20),\n",
    "                        'min_samples_leaf': randint(1, 10)\n",
    "                    }\n",
    "                },\n",
    "                'GradientBoosting': {\n",
    "                    'model': GradientBoostingClassifier(random_state=42),\n",
    "                    'params': {\n",
    "                        'n_estimators': randint(50, 200),\n",
    "                        'learning_rate': uniform(0.01, 0.3),\n",
    "                        'max_depth': randint(3, 10),\n",
    "                        'subsample': uniform(0.6, 0.4)\n",
    "                    }\n",
    "                },\n",
    "                'LogisticRegression': {\n",
    "                    'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "                    'params': {\n",
    "                        'C': uniform(0.01, 10),\n",
    "                        'penalty': ['l1', 'l2'],\n",
    "                        'solver': ['liblinear']\n",
    "                    }\n",
    "                },\n",
    "                'SVM': {\n",
    "                    'model': SVC(random_state=42, probability=True),\n",
    "                    'params': {\n",
    "                        'C': uniform(0.1, 10),\n",
    "                        'kernel': ['rbf', 'linear'],\n",
    "                        'gamma': ['scale', 'auto']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        else:  # regression\n",
    "            from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "            from sklearn.linear_model import LinearRegression, Ridge\n",
    "            from sklearn.svm import SVR\n",
    "            \n",
    "            self.models = {\n",
    "                'RandomForest': {\n",
    "                    'model': RandomForestRegressor(random_state=42),\n",
    "                    'params': {\n",
    "                        'n_estimators': randint(50, 200),\n",
    "                        'max_depth': randint(3, 20),\n",
    "                        'min_samples_split': randint(2, 20)\n",
    "                    }\n",
    "                },\n",
    "                'GradientBoosting': {\n",
    "                    'model': GradientBoostingRegressor(random_state=42),\n",
    "                    'params': {\n",
    "                        'n_estimators': randint(50, 200),\n",
    "                        'learning_rate': uniform(0.01, 0.3),\n",
    "                        'max_depth': randint(3, 10)\n",
    "                    }\n",
    "                },\n",
    "                'Ridge': {\n",
    "                    'model': Ridge(random_state=42),\n",
    "                    'params': {\n",
    "                        'alpha': uniform(0.01, 10)\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit AutoML pipeline\"\"\"\n",
    "        print(f\"ü§ñ Starting AutoML for {self.task_type} task...\")\n",
    "        print(f\"‚è±Ô∏è Time limit: {self.time_limit} seconds\")\n",
    "        print(f\"üîÑ Cross-validation folds: {self.cv_folds}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Try each model\n",
    "        for model_name, model_config in self.models.items():\n",
    "            if time.time() - start_time > self.time_limit:\n",
    "                print(f\"‚è∞ Time limit reached. Stopping at {model_name}\")\n",
    "                break\n",
    "            \n",
    "            print(f\"\\nüîß Tuning {model_name}...\")\n",
    "            \n",
    "            # Hyperparameter tuning\n",
    "            search = RandomizedSearchCV(\n",
    "                model_config['model'],\n",
    "                model_config['params'],\n",
    "                n_iter=20,\n",
    "                cv=self.cv_folds,\n",
    "                scoring='accuracy' if self.task_type == 'classification' else 'neg_mean_squared_error',\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            search.fit(X_scaled, y)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'model_name': model_name,\n",
    "                'best_model': search.best_estimator_,\n",
    "                'best_score': search.best_score_,\n",
    "                'best_params': search.best_params_,\n",
    "                'cv_results': search.cv_results_\n",
    "            }\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            print(f\"‚úÖ {model_name} - Best CV Score: {search.best_score_:.4f}\")\n",
    "            \n",
    "            # Update best model\n",
    "            if self.best_model is None or search.best_score_ > self.best_score:\n",
    "                self.best_model = search.best_estimator_\n",
    "                self.best_score = search.best_score_\n",
    "                self.best_model_name = model_name\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nüéâ AutoML completed in {total_time:.1f} seconds!\")\n",
    "        print(f\"üèÜ Best model: {self.best_model_name}\")\n",
    "        print(f\"üìä Best CV score: {self.best_score:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"Model not fitted yet!\")\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.best_model.predict(X_scaled)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities (classification only)\"\"\"\n",
    "        if self.task_type != 'classification':\n",
    "            raise ValueError(\"predict_proba only available for classification\")\n",
    "        \n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"Model not fitted yet!\")\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.best_model.predict_proba(X_scaled)\n",
    "    \n",
    "    def get_leaderboard(self):\n",
    "        \"\"\"Get model performance leaderboard\"\"\"\n",
    "        leaderboard = pd.DataFrame([\n",
    "            {\n",
    "                'Model': result['model_name'],\n",
    "                'CV_Score': result['best_score'],\n",
    "                'Best_Params': str(result['best_params'])\n",
    "            }\n",
    "            for result in self.results\n",
    "        ]).sort_values('CV_Score', ascending=False)\n",
    "        \n",
    "        return leaderboard\n",
    "\n",
    "print(\"‚úÖ SimpleAutoML class defined!\")\n",
    "print(\"ü§ñ Ready to automate machine learning!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
